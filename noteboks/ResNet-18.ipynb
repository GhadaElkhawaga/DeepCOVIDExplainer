{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.load('data/x_train.npy')\n",
    "y_train = np.load('data/y_train.npy')\n",
    "x_test = np.load('data/x_test.npy')\n",
    "y_test = np.load('data/y_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654, 224, 224, 3)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x_train /= 255????"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "import time\n",
    "import csv\n",
    "from PIL import Image\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from keras import initializers\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing import sequence\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential,load_model,Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import *\n",
    "from keras.callbacks import CSVLogger\n",
    "from keras import callbacks\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import sklearn.metrics as sklm\n",
    "import lossprettifier\n",
    "from ResNet import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reproducibility\n",
    "np.random.seed(3768)\n",
    "\n",
    "# use this environment flag to change which GPU to use \n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"  # specify which GPU(s) to be used\n",
    "\n",
    "#Get TensorFlow session\n",
    "def get_session(): \n",
    "  config = tf.ConfigProto() \n",
    "  config.gpu_options.allow_growth = True \n",
    "  return tf.Session(config=config) \n",
    "  \n",
    "# One hot encoding of labels \n",
    "def dense_to_one_hot(labels_dense,num_clases=4):\n",
    "  return np.eye(num_clases)[labels_dense]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing training and test sets\n",
    "x_train, x_valid, y_train, y_valid = train_test_split(x_train, y_train, test_size=0.10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = dense_to_one_hot(y_train,num_clases=4)\n",
    "y_valid= dense_to_one_hot(y_valid,num_clases=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reza/.local/lib/python3.6/site-packages/keras_preprocessing/image/image_data_generator.py:348: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    }
   ],
   "source": [
    "#Image data generation for the training \n",
    "datagen = ImageDataGenerator(\n",
    "               featurewise_center = False, \n",
    "               samplewise_center = False,  # set each sample mean to 0\n",
    "               featurewise_std_normalization = True,  \n",
    "               samplewise_std_normalization = False)  \n",
    "\n",
    "datagen.fit(x_train) \n",
    "for i in range(len(x_test)):\n",
    "      x_test[i] = datagen.standardize(x_test[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reshaping via a convolution...\n",
      "reshaping via a convolution...\n",
      "reshaping via a convolution...\n",
      "reshaping via a convolution...\n",
      "Epoch 1/50\n",
      "32/32 [==============================] - 78s 2s/step - loss: 3.8593 - accuracy: 0.6270 - val_loss: 1130.2300 - val_accuracy: 0.2539\n",
      "Epoch 2/50\n",
      "32/32 [==============================] - 73s 2s/step - loss: 3.6147 - accuracy: 0.7143 - val_loss: 286.7065 - val_accuracy: 0.2704\n",
      "Epoch 3/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 3.5902 - accuracy: 0.7178 - val_loss: 102.1051 - val_accuracy: 0.2684\n",
      "Epoch 4/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 3.5250 - accuracy: 0.7393 - val_loss: 26.7099 - val_accuracy: 0.2505\n",
      "Epoch 5/50\n",
      "32/32 [==============================] - 75s 2s/step - loss: 3.4561 - accuracy: 0.7832 - val_loss: 14.2865 - val_accuracy: 0.2744\n",
      "Epoch 6/50\n",
      "32/32 [==============================] - 73s 2s/step - loss: 3.4043 - accuracy: 0.7695 - val_loss: 9.3615 - val_accuracy: 0.2704\n",
      "Epoch 7/50\n",
      "32/32 [==============================] - 73s 2s/step - loss: 3.4066 - accuracy: 0.7871 - val_loss: 13.1856 - val_accuracy: 0.2584\n",
      "Epoch 8/50\n",
      "32/32 [==============================] - 73s 2s/step - loss: 3.4122 - accuracy: 0.7852 - val_loss: 5.2803 - val_accuracy: 0.3579\n",
      "Epoch 9/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 3.3905 - accuracy: 0.7861 - val_loss: 3.5473 - val_accuracy: 0.7276\n",
      "Epoch 10/50\n",
      "32/32 [==============================] - 73s 2s/step - loss: 3.3387 - accuracy: 0.7959 - val_loss: 3.8871 - val_accuracy: 0.6044\n",
      "Epoch 11/50\n",
      "32/32 [==============================] - 75s 2s/step - loss: 3.3018 - accuracy: 0.8018 - val_loss: 3.6547 - val_accuracy: 0.7038\n",
      "Epoch 12/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 3.3143 - accuracy: 0.8086 - val_loss: 3.3349 - val_accuracy: 0.7555\n",
      "Epoch 13/50\n",
      "32/32 [==============================] - 73s 2s/step - loss: 3.2848 - accuracy: 0.8132 - val_loss: 3.4284 - val_accuracy: 0.6859\n",
      "Epoch 14/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 3.3172 - accuracy: 0.8018 - val_loss: 3.8279 - val_accuracy: 0.5209\n",
      "Epoch 15/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 3.2460 - accuracy: 0.8340 - val_loss: 3.1901 - val_accuracy: 0.7694\n",
      "Epoch 16/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 3.2157 - accuracy: 0.8412 - val_loss: 3.4472 - val_accuracy: 0.7535\n",
      "Epoch 17/50\n",
      "32/32 [==============================] - 75s 2s/step - loss: 3.2261 - accuracy: 0.8232 - val_loss: 3.3914 - val_accuracy: 0.7475\n",
      "Epoch 18/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 3.2335 - accuracy: 0.8320 - val_loss: 3.5598 - val_accuracy: 0.7383\n",
      "Epoch 19/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 3.2080 - accuracy: 0.8291 - val_loss: 3.3610 - val_accuracy: 0.7455\n",
      "Epoch 20/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 3.1325 - accuracy: 0.8760 - val_loss: 3.7209 - val_accuracy: 0.7217\n",
      "Epoch 21/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 3.1284 - accuracy: 0.8631 - val_loss: 3.4455 - val_accuracy: 0.7495\n",
      "Epoch 22/50\n",
      "32/32 [==============================] - 75s 2s/step - loss: 3.1604 - accuracy: 0.8496 - val_loss: 3.0686 - val_accuracy: 0.7714\n",
      "Epoch 23/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 3.1449 - accuracy: 0.8369 - val_loss: 3.9068 - val_accuracy: 0.7515\n",
      "Epoch 24/50\n",
      "32/32 [==============================] - 70s 2s/step - loss: 3.1027 - accuracy: 0.8682 - val_loss: 3.1616 - val_accuracy: 0.7594\n",
      "Epoch 25/50\n",
      "32/32 [==============================] - 70s 2s/step - loss: 3.0319 - accuracy: 0.8955 - val_loss: 8.5852 - val_accuracy: 0.3161\n",
      "Epoch 26/50\n",
      "32/32 [==============================] - 69s 2s/step - loss: 3.0787 - accuracy: 0.8838 - val_loss: 3.2822 - val_accuracy: 0.7634\n",
      "Epoch 27/50\n",
      "32/32 [==============================] - 70s 2s/step - loss: 3.0778 - accuracy: 0.8741 - val_loss: 3.7060 - val_accuracy: 0.7575\n",
      "Epoch 28/50\n",
      "32/32 [==============================] - 72s 2s/step - loss: 3.0845 - accuracy: 0.8721 - val_loss: 3.4433 - val_accuracy: 0.7217\n",
      "Epoch 29/50\n",
      "32/32 [==============================] - 73s 2s/step - loss: 3.0260 - accuracy: 0.8906 - val_loss: 6.7136 - val_accuracy: 0.3579\n",
      "Epoch 30/50\n",
      "32/32 [==============================] - 72s 2s/step - loss: 2.9881 - accuracy: 0.8945 - val_loss: 3.2591 - val_accuracy: 0.7495\n",
      "Epoch 31/50\n",
      "32/32 [==============================] - 73s 2s/step - loss: 2.9370 - accuracy: 0.9258 - val_loss: 3.5817 - val_accuracy: 0.7217\n",
      "Epoch 32/50\n",
      "32/32 [==============================] - 72s 2s/step - loss: 2.9795 - accuracy: 0.8955 - val_loss: 3.2548 - val_accuracy: 0.7416\n",
      "Epoch 33/50\n",
      "32/32 [==============================] - 72s 2s/step - loss: 3.0388 - accuracy: 0.8871 - val_loss: 3.3379 - val_accuracy: 0.7455\n",
      "Epoch 34/50\n",
      "32/32 [==============================] - 72s 2s/step - loss: 2.9381 - accuracy: 0.9141 - val_loss: 3.5816 - val_accuracy: 0.7694\n",
      "Epoch 35/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 2.8870 - accuracy: 0.9355 - val_loss: 3.3679 - val_accuracy: 0.7695\n",
      "Epoch 36/50\n",
      "32/32 [==============================] - 75s 2s/step - loss: 2.9027 - accuracy: 0.9248 - val_loss: 4.0747 - val_accuracy: 0.7992\n",
      "Epoch 37/50\n",
      "32/32 [==============================] - 72s 2s/step - loss: 2.8682 - accuracy: 0.9401 - val_loss: 3.4795 - val_accuracy: 0.7833\n",
      "Epoch 38/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 2.8957 - accuracy: 0.9229 - val_loss: 4.4265 - val_accuracy: 0.6441\n",
      "Epoch 39/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 2.8088 - accuracy: 0.9551 - val_loss: 4.0555 - val_accuracy: 0.7336\n",
      "Epoch 40/50\n",
      "32/32 [==============================] - 73s 2s/step - loss: 2.9258 - accuracy: 0.9141 - val_loss: 5.3826 - val_accuracy: 0.4692\n",
      "Epoch 41/50\n",
      "32/32 [==============================] - 74s 2s/step - loss: 2.9288 - accuracy: 0.9033 - val_loss: 6.1204 - val_accuracy: 0.4970\n",
      "Epoch 42/50\n",
      "32/32 [==============================] - 73s 2s/step - loss: 2.9852 - accuracy: 0.8730 - val_loss: 3.9059 - val_accuracy: 0.7256\n",
      "Epoch 43/50\n",
      "32/32 [==============================] - 72s 2s/step - loss: 2.8609 - accuracy: 0.9287 - val_loss: 3.8044 - val_accuracy: 0.7555\n",
      "Epoch 44/50\n",
      "32/32 [==============================] - 72s 2s/step - loss: 2.7896 - accuracy: 0.9540 - val_loss: 3.2766 - val_accuracy: 0.7654\n",
      "Epoch 45/50\n",
      "11/32 [=========>....................] - ETA: 45s - loss: 2.7915 - accuracy: 0.9489"
     ]
    }
   ],
   "source": [
    "#Defining hyperparameters\n",
    "batch_Size = 32\n",
    "steps_Per_Epoch = 32\n",
    "numEpochs = 50\n",
    "\n",
    "#Instantating ResNet18 model\n",
    "model = ResNet18((224, 224, 3), 4) \n",
    "\n",
    "#Creating an optimizers\n",
    "adaDelta = keras.optimizers.Adadelta(lr=1.0, rho=0.95)\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.95, nesterov=True)\n",
    "model.compile(optimizer = sgd , loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "#Creating early stopping \n",
    "earlystop = EarlyStopping(monitor = 'val_accuracy', min_delta = 0, patience = 50, verbose = 1, mode = 'auto', restore_best_weights = True)       \n",
    "\n",
    "train_generator = datagen.flow(x_train, y_train, batch_size = batch_Size)\n",
    "validation_generator = datagen.flow(x_valid, y_valid, batch_size = batch_Size)\n",
    "\n",
    "# Model training\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = steps_Per_Epoch,\n",
    "    validation_data = validation_generator, \n",
    "    validation_steps = 16,\n",
    "    epochs = numEpochs,\n",
    "    shuffle = True, \n",
    "    verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelPath = \"ResNet18_COVID19.h5\"\n",
    "resultPath = 'ResNet18_COVID19.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch     0 | LossA: 3.82(+0.00%) \u001b[0m\t| LossAB: 1515.01(+0.00%) \u001b[0m\t\n",
      "Epoch     1 | LossA: \u001b[32m3.61(-5.36%) ▼\u001b[0m\t| LossAB: \u001b[32m293.95(-80.60%) ▼\u001b[0m\t\n",
      "Epoch     2 | LossA: \u001b[32m3.59(-0.76%) ▼\u001b[0m\t| LossAB: \u001b[32m66.91(-77.24%) ▼\u001b[0m\t\n",
      "Epoch     3 | LossA: \u001b[32m3.49(-2.55%) ▼\u001b[0m\t| LossAB: \u001b[32m29.26(-56.27%) ▼\u001b[0m\t\n",
      "Epoch     4 | LossA: \u001b[32m3.48(-0.42%) ▼\u001b[0m\t| LossAB: \u001b[32m14.31(-51.10%) ▼\u001b[0m\t\n",
      "Epoch     5 | LossA: \u001b[32m3.40(-2.45%) ▼\u001b[0m\t| LossAB: \u001b[32m5.35(-62.63%) ▼\u001b[0m\t\n",
      "Epoch     6 | LossA: \u001b[91m3.42(+0.86%) ▲\u001b[0m\t| LossAB: \u001b[32m5.03(-5.97%) ▼\u001b[0m\t\n",
      "Epoch     7 | LossA: \u001b[91m3.43(+0.21%) ▲\u001b[0m\t| LossAB: \u001b[91m7.25(+44.15%) ▲\u001b[0m\t\n",
      "Epoch     8 | LossA: \u001b[32m3.41(-0.69%) ▼\u001b[0m\t| LossAB: \u001b[91m8.45(+16.64%) ▲\u001b[0m\t\n",
      "Epoch     9 | LossA: \u001b[32m3.35(-1.62%) ▼\u001b[0m\t| LossAB: \u001b[32m3.71(-56.15%) ▼\u001b[0m\t\n",
      "Epoch    10 | LossA: \u001b[32m3.32(-1.03%) ▼\u001b[0m\t| LossAB: \u001b[91m4.33(+16.90%) ▲\u001b[0m\t\n",
      "Epoch    11 | LossA: \u001b[32m3.31(-0.14%) ▼\u001b[0m\t| LossAB: \u001b[32m4.06(-6.24%) ▼\u001b[0m\t\n",
      "Epoch    12 | LossA: \u001b[32m3.31(-0.17%) ▼\u001b[0m\t| LossAB: \u001b[32m3.67(-9.78%) ▼\u001b[0m\t\n",
      "Epoch    13 | LossA: \u001b[91m3.32(+0.32%) ▲\u001b[0m\t| LossAB: \u001b[91m3.74(+1.93%) ▲\u001b[0m\t\n",
      "Epoch    14 | LossA: \u001b[32m3.21(-3.14%) ▼\u001b[0m\t| LossAB: \u001b[32m3.45(-7.80%) ▼\u001b[0m\t\n",
      "Epoch    15 | LossA: \u001b[91m3.22(+0.32%) ▲\u001b[0m\t| LossAB: \u001b[32m3.31(-3.85%) ▼\u001b[0m\t\n",
      "Epoch    16 | LossA: \u001b[91m3.23(+0.07%) ▲\u001b[0m\t| LossAB: \u001b[91m3.36(+1.49%) ▲\u001b[0m\t\n",
      "Epoch    17 | LossA: \u001b[91m3.23(+0.18%) ▲\u001b[0m\t| LossAB: \u001b[91m3.88(+15.33%) ▲\u001b[0m\t\n",
      "Epoch    18 | LossA: \u001b[32m3.21(-0.67%) ▼\u001b[0m\t| LossAB: \u001b[91m4.27(+10.18%) ▲\u001b[0m\t\n",
      "Epoch    19 | LossA: \u001b[32m3.12(-2.97%) ▼\u001b[0m\t| LossAB: \u001b[32m3.33(-22.06%) ▼\u001b[0m\t\n",
      "Epoch    20 | LossA: \u001b[91m3.13(+0.54%) ▲\u001b[0m\t| LossAB: \u001b[91m3.72(+11.81%) ▲\u001b[0m\t\n",
      "Epoch    21 | LossA: \u001b[32m3.10(-0.88%) ▼\u001b[0m\t| LossAB: \u001b[91m4.98(+33.81%) ▲\u001b[0m\t\n",
      "Epoch    22 | LossA: \u001b[91m3.16(+1.81%) ▲\u001b[0m\t| LossAB: \u001b[32m4.05(-18.67%) ▼\u001b[0m\t\n",
      "Epoch    23 | LossA: \u001b[32m3.09(-2.11%) ▼\u001b[0m\t| LossAB: \u001b[32m3.81(-5.93%) ▼\u001b[0m\t\n",
      "Epoch    24 | LossA: \u001b[32m3.05(-1.53%) ▼\u001b[0m\t| LossAB: \u001b[32m3.80(-0.33%) ▼\u001b[0m\t\n",
      "Epoch    25 | LossA: \u001b[91m3.05(+0.07%) ▲\u001b[0m\t| LossAB: \u001b[32m3.14(-17.29%) ▼\u001b[0m\t\n",
      "Epoch    26 | LossA: \u001b[91m3.12(+2.29%) ▲\u001b[0m\t| LossAB: \u001b[91m7.82(+149.01%) ▲\u001b[0m\t\n",
      "Epoch    27 | LossA: \u001b[32m3.11(-0.40%) ▼\u001b[0m\t| LossAB: \u001b[32m3.29(-57.95%) ▼\u001b[0m\t\n",
      "Epoch    28 | LossA: \u001b[32m3.06(-1.60%) ▼\u001b[0m\t| LossAB: \u001b[91m3.49(+6.19%) ▲\u001b[0m\t\n",
      "Epoch    29 | LossA: \u001b[32m2.95(-3.33%) ▼\u001b[0m\t| LossAB: \u001b[91m3.87(+10.78%) ▲\u001b[0m\t\n",
      "Epoch    30 | LossA: \u001b[91m2.96(+0.08%) ▲\u001b[0m\t| LossAB: \u001b[91m4.56(+17.81%) ▲\u001b[0m\t\n",
      "Epoch    31 | LossA: \u001b[91m3.00(+1.32%) ▲\u001b[0m\t| LossAB: \u001b[91m9.76(+113.97%) ▲\u001b[0m\t\n",
      "Epoch    32 | LossA: \u001b[91m3.03(+1.06%) ▲\u001b[0m\t| LossAB: \u001b[32m4.85(-50.29%) ▼\u001b[0m\t\n",
      "Epoch    33 | LossA: \u001b[32m2.97(-1.97%) ▼\u001b[0m\t| LossAB: \u001b[32m3.52(-27.45%) ▼\u001b[0m\t\n",
      "Epoch    34 | LossA: \u001b[32m2.93(-1.31%) ▼\u001b[0m\t| LossAB: \u001b[32m3.31(-5.88%) ▼\u001b[0m\t\n",
      "Epoch    35 | LossA: \u001b[91m2.94(+0.22%) ▲\u001b[0m\t| LossAB: \u001b[91m3.69(+11.55%) ▲\u001b[0m\t\n",
      "Epoch    36 | LossA: \u001b[91m3.02(+2.93%) ▲\u001b[0m\t| LossAB: \u001b[32m3.44(-6.77%) ▼\u001b[0m\t\n",
      "Epoch    37 | LossA: \u001b[32m2.89(-4.23%) ▼\u001b[0m\t| LossAB: \u001b[91m3.50(+1.54%) ▲\u001b[0m\t\n",
      "Epoch    38 | LossA: \u001b[32m2.84(-1.83%) ▼\u001b[0m\t| LossAB: \u001b[91m4.12(+17.77%) ▲\u001b[0m\t\n",
      "Epoch    39 | LossA: \u001b[32m2.84(-0.19%) ▼\u001b[0m\t| LossAB: \u001b[91m4.78(+16.15%) ▲\u001b[0m\t\n",
      "Epoch    40 | LossA: \u001b[32m2.82(-0.63%) ▼\u001b[0m\t| LossAB: \u001b[32m3.61(-24.54%) ▼\u001b[0m\t\n",
      "Epoch    41 | LossA: \u001b[91m2.84(+0.70%) ▲\u001b[0m\t| LossAB: \u001b[32m3.32(-7.92%) ▼\u001b[0m\t\n",
      "Epoch    42 | LossA: \u001b[32m2.78(-2.10%) ▼\u001b[0m\t| LossAB: \u001b[91m3.41(+2.56%) ▲\u001b[0m\t\n",
      "Epoch    43 | LossA: \u001b[91m2.78(+0.21%) ▲\u001b[0m\t| LossAB: \u001b[91m5.48(+60.87%) ▲\u001b[0m\t\n",
      "Epoch    44 | LossA: \u001b[32m2.77(-0.52%) ▼\u001b[0m\t| LossAB: \u001b[32m3.44(-37.22%) ▼\u001b[0m\t\n",
      "Epoch    45 | LossA: \u001b[91m2.80(+1.26%) ▲\u001b[0m\t| LossAB: \u001b[91m3.45(+0.32%) ▲\u001b[0m\t\n",
      "Epoch    46 | LossA: \u001b[91m2.81(+0.26%) ▲\u001b[0m\t| LossAB: \u001b[32m3.37(-2.38%) ▼\u001b[0m\t\n",
      "Epoch    47 | LossA: \u001b[32m2.74(-2.48%) ▼\u001b[0m\t| LossAB: \u001b[91m3.98(+18.11%) ▲\u001b[0m\t\n",
      "Epoch    48 | LossA: \u001b[32m2.73(-0.48%) ▼\u001b[0m\t| LossAB: \u001b[32m3.23(-19.01%) ▼\u001b[0m\t\n",
      "Epoch    49 | LossA: \u001b[91m2.75(+0.81%) ▲\u001b[0m\t| LossAB: \u001b[91m3.71(+15.05%) ▲\u001b[0m\t\n",
      "Epoch    50 | LossA: \u001b[32m2.72(-1.12%) ▼\u001b[0m\t| LossAB: \u001b[91m4.69(+26.48%) ▲\u001b[0m\t\n",
      "Epoch    51 | LossA: \u001b[91m2.74(+0.72%) ▲\u001b[0m\t| LossAB: \u001b[32m3.51(-25.30%) ▼\u001b[0m\t\n",
      "Epoch    52 | LossA: \u001b[32m2.67(-2.47%) ▼\u001b[0m\t| LossAB: \u001b[91m3.52(+0.32%) ▲\u001b[0m\t\n",
      "Epoch    53 | LossA: \u001b[91m2.68(+0.22%) ▲\u001b[0m\t| LossAB: \u001b[32m3.35(-4.84%) ▼\u001b[0m\t\n",
      "Epoch    54 | LossA: \u001b[32m2.67(-0.44%) ▼\u001b[0m\t| LossAB: \u001b[91m3.70(+10.65%) ▲\u001b[0m\t\n",
      "Epoch    55 | LossA: \u001b[32m2.66(-0.13%) ▼\u001b[0m\t| LossAB: \u001b[32m3.44(-7.20%) ▼\u001b[0m\t\n",
      "Epoch    56 | LossA: \u001b[32m2.65(-0.47%) ▼\u001b[0m\t| LossAB: \u001b[32m3.12(-9.08%) ▼\u001b[0m\t\n",
      "Epoch    57 | LossA: \u001b[32m2.62(-0.97%) ▼\u001b[0m\t| LossAB: \u001b[91m6.54(+109.42%) ▲\u001b[0m\t\n",
      "Epoch    58 | LossA: \u001b[91m2.65(+1.13%) ▲\u001b[0m\t| LossAB: \u001b[32m3.99(-39.06%) ▼\u001b[0m\t\n",
      "Epoch    59 | LossA: \u001b[91m2.68(+1.08%) ▲\u001b[0m\t| LossAB: \u001b[91m4.03(+0.96%) ▲\u001b[0m\t\n",
      "Epoch    60 | LossA: \u001b[32m2.67(-0.33%) ▼\u001b[0m\t| LossAB: \u001b[91m6.35(+57.78%) ▲\u001b[0m\t\n",
      "Epoch    61 | LossA: \u001b[32m2.64(-1.40%) ▼\u001b[0m\t| LossAB: \u001b[32m3.07(-51.59%) ▼\u001b[0m\t\n",
      "Epoch    62 | LossA: \u001b[91m2.64(+0.09%) ▲\u001b[0m\t| LossAB: \u001b[91m4.74(+54.00%) ▲\u001b[0m\t\n",
      "Epoch    63 | LossA: \u001b[32m2.62(-0.67%) ▼\u001b[0m\t| LossAB: \u001b[32m3.75(-20.81%) ▼\u001b[0m\t\n",
      "Epoch    64 | LossA: \u001b[91m2.64(+0.69%) ▲\u001b[0m\t| LossAB: \u001b[32m3.18(-15.07%) ▼\u001b[0m\t\n",
      "Epoch    65 | LossA: \u001b[32m2.62(-0.80%) ▼\u001b[0m\t| LossAB: \u001b[91m3.21(+0.88%) ▲\u001b[0m\t\n",
      "Epoch    66 | LossA: \u001b[32m2.59(-0.89%) ▼\u001b[0m\t| LossAB: \u001b[91m4.46(+38.81%) ▲\u001b[0m\t\n",
      "Epoch    67 | LossA: \u001b[32m2.57(-0.97%) ▼\u001b[0m\t| LossAB: \u001b[32m4.21(-5.53%) ▼\u001b[0m\t\n",
      "Epoch    68 | LossA: \u001b[91m2.58(+0.37%) ▲\u001b[0m\t| LossAB: \u001b[91m4.51(+6.95%) ▲\u001b[0m\t\n",
      "Epoch    69 | LossA: \u001b[32m2.57(-0.17%) ▼\u001b[0m\t| LossAB: \u001b[32m3.42(-24.02%) ▼\u001b[0m\t\n",
      "Epoch    70 | LossA: \u001b[32m2.55(-0.99%) ▼\u001b[0m\t| LossAB: \u001b[91m3.76(+9.72%) ▲\u001b[0m\t\n",
      "Epoch    71 | LossA: \u001b[91m2.57(+1.02%) ▲\u001b[0m\t| LossAB: \u001b[91m4.42(+17.55%) ▲\u001b[0m\t\n",
      "Epoch    72 | LossA: \u001b[32m2.57(-0.12%) ▼\u001b[0m\t| LossAB: \u001b[32m4.04(-8.55%) ▼\u001b[0m\t\n",
      "Epoch    73 | LossA: \u001b[91m2.59(+0.69%) ▲\u001b[0m\t| LossAB: \u001b[32m2.86(-29.15%) ▼\u001b[0m\t\n",
      "Epoch    74 | LossA: \u001b[32m2.55(-1.48%) ▼\u001b[0m\t| LossAB: \u001b[91m4.66(+62.79%) ▲\u001b[0m\t\n",
      "Epoch    75 | LossA: \u001b[32m2.53(-0.79%) ▼\u001b[0m\t| LossAB: \u001b[32m3.42(-26.57%) ▼\u001b[0m\t\n",
      "Epoch    76 | LossA: \u001b[91m2.54(+0.18%) ▲\u001b[0m\t| LossAB: \u001b[91m4.01(+17.38%) ▲\u001b[0m\t\n",
      "Epoch    77 | LossA: \u001b[32m2.52(-0.70%) ▼\u001b[0m\t| LossAB: \u001b[32m3.28(-18.35%) ▼\u001b[0m\t\n",
      "Epoch    78 | LossA: \u001b[91m2.52(+0.09%) ▲\u001b[0m\t| LossAB: \u001b[91m3.29(+0.35%) ▲\u001b[0m\t\n",
      "Epoch    79 | LossA: \u001b[32m2.50(-0.90%) ▼\u001b[0m\t| LossAB: \u001b[91m3.88(+17.82%) ▲\u001b[0m\t\n",
      "Epoch    80 | LossA: \u001b[32m2.49(-0.39%) ▼\u001b[0m\t| LossAB: \u001b[91m4.00(+3.28%) ▲\u001b[0m\t\n",
      "Epoch    81 | LossA: \u001b[32m2.47(-0.64%) ▼\u001b[0m\t| LossAB: \u001b[32m2.85(-28.78%) ▼\u001b[0m\t\n",
      "Epoch    82 | LossA: \u001b[32m2.47(-0.21%) ▼\u001b[0m\t| LossAB: \u001b[91m3.26(+14.44%) ▲\u001b[0m\t\n",
      "Epoch    83 | LossA: \u001b[91m2.47(+0.27%) ▲\u001b[0m\t| LossAB: \u001b[91m3.94(+20.65%) ▲\u001b[0m\t\n",
      "Epoch    84 | LossA: \u001b[32m2.46(-0.73%) ▼\u001b[0m\t| LossAB: \u001b[91m3.98(+1.18%) ▲\u001b[0m\t\n",
      "Epoch    85 | LossA: \u001b[32m2.45(-0.03%) ▼\u001b[0m\t| LossAB: \u001b[32m3.62(-9.00%) ▼\u001b[0m\t\n",
      "Epoch    86 | LossA: \u001b[32m2.44(-0.57%) ▼\u001b[0m\t| LossAB: \u001b[32m3.44(-5.09%) ▼\u001b[0m\t\n",
      "Epoch    87 | LossA: \u001b[91m2.45(+0.29%) ▲\u001b[0m\t| LossAB: \u001b[91m4.96(+44.28%) ▲\u001b[0m\t\n",
      "Epoch    88 | LossA: \u001b[32m2.44(-0.19%) ▼\u001b[0m\t| LossAB: \u001b[32m3.58(-27.96%) ▼\u001b[0m\t\n",
      "Epoch    89 | LossA: \u001b[32m2.43(-0.38%) ▼\u001b[0m\t| LossAB: \u001b[91m4.10(+14.55%) ▲\u001b[0m\t\n",
      "Epoch    90 | LossA: \u001b[91m2.44(+0.35%) ▲\u001b[0m\t| LossAB: \u001b[32m3.65(-10.96%) ▼\u001b[0m\t\n",
      "Epoch    91 | LossA: \u001b[91m2.45(+0.21%) ▲\u001b[0m\t| LossAB: \u001b[32m3.09(-15.24%) ▼\u001b[0m\t\n",
      "Epoch    92 | LossA: \u001b[91m2.46(+0.45%) ▲\u001b[0m\t| LossAB: \u001b[91m3.31(+6.99%) ▲\u001b[0m\t\n",
      "Epoch    93 | LossA: \u001b[32m2.44(-0.90%) ▼\u001b[0m\t| LossAB: \u001b[32m3.17(-4.00%) ▼\u001b[0m\t\n",
      "Epoch    94 | LossA: \u001b[91m2.48(+1.66%) ▲\u001b[0m\t| LossAB: \u001b[91m4.07(+28.08%) ▲\u001b[0m\t\n",
      "Epoch    95 | LossA: \u001b[32m2.47(-0.29%) ▼\u001b[0m\t| LossAB: \u001b[91m7.56(+86.01%) ▲\u001b[0m\t\n",
      "Epoch    96 | LossA: \u001b[32m2.46(-0.30%) ▼\u001b[0m\t| LossAB: \u001b[32m5.02(-33.60%) ▼\u001b[0m\t\n",
      "Epoch    97 | LossA: \u001b[32m2.43(-1.13%) ▼\u001b[0m\t| LossAB: \u001b[32m3.67(-26.98%) ▼\u001b[0m\t\n",
      "Epoch    98 | LossA: \u001b[91m2.44(+0.40%) ▲\u001b[0m\t| LossAB: \u001b[32m2.78(-24.13%) ▼\u001b[0m\t\n",
      "Epoch    99 | LossA: \u001b[32m2.40(-1.65%) ▼\u001b[0m\t| LossAB: \u001b[91m3.61(+29.67%) ▲\u001b[0m\t\n",
      "Epoch   100 | LossA: \u001b[32m2.38(-0.90%) ▼\u001b[0m\t| LossAB: \u001b[32m2.78(-23.04%) ▼\u001b[0m\t\n",
      "Epoch   101 | LossA: \u001b[91m2.38(+0.05%) ▲\u001b[0m\t| LossAB: \u001b[91m3.51(+26.40%) ▲\u001b[0m\t\n",
      "Epoch   102 | LossA: \u001b[91m2.48(+3.90%) ▲\u001b[0m\t| LossAB: \u001b[32m3.33(-5.17%) ▼\u001b[0m\t\n",
      "Epoch   103 | LossA: \u001b[32m2.39(-3.45%) ▼\u001b[0m\t| LossAB: \u001b[91m6.08(+82.79%) ▲\u001b[0m\t\n",
      "Epoch   104 | LossA: \u001b[32m2.38(-0.33%) ▼\u001b[0m\t| LossAB: \u001b[32m3.24(-46.72%) ▼\u001b[0m\t\n",
      "Epoch   105 | LossA: \u001b[91m2.39(+0.46%) ▲\u001b[0m\t| LossAB: \u001b[91m4.13(+27.56%) ▲\u001b[0m\t\n",
      "Epoch   106 | LossA: \u001b[32m2.36(-1.54%) ▼\u001b[0m\t| LossAB: \u001b[32m4.04(-2.24%) ▼\u001b[0m\t\n",
      "Epoch   107 | LossA: \u001b[91m2.37(+0.65%) ▲\u001b[0m\t| LossAB: \u001b[32m4.01(-0.68%) ▼\u001b[0m\t\n",
      "Epoch   108 | LossA: \u001b[32m2.33(-1.75%) ▼\u001b[0m\t| LossAB: \u001b[32m3.06(-23.68%) ▼\u001b[0m\t\n",
      "Epoch   109 | LossA: \u001b[91m2.34(+0.35%) ▲\u001b[0m\t| LossAB: \u001b[91m3.41(+11.29%) ▲\u001b[0m\t\n",
      "Epoch   110 | LossA: \u001b[91m2.34(+0.09%) ▲\u001b[0m\t| LossAB: \u001b[91m3.93(+15.15%) ▲\u001b[0m\t\n",
      "Epoch   111 | LossA: \u001b[91m2.38(+1.87%) ▲\u001b[0m\t| LossAB: \u001b[91m4.78(+21.77%) ▲\u001b[0m\t\n",
      "Epoch   112 | LossA: \u001b[32m2.35(-1.25%) ▼\u001b[0m\t| LossAB: \u001b[32m2.91(-39.14%) ▼\u001b[0m\t\n",
      "Epoch   113 | LossA: \u001b[32m2.32(-1.27%) ▼\u001b[0m\t| LossAB: \u001b[91m3.15(+8.31%) ▲\u001b[0m\t\n",
      "Epoch   114 | LossA: \u001b[32m2.30(-1.20%) ▼\u001b[0m\t| LossAB: \u001b[91m3.60(+14.34%) ▲\u001b[0m\t\n",
      "Epoch   115 | LossA: \u001b[91m2.31(+0.60%) ▲\u001b[0m\t| LossAB: \u001b[32m3.41(-5.42%) ▼\u001b[0m\t\n",
      "Epoch   116 | LossA: \u001b[32m2.29(-0.68%) ▼\u001b[0m\t| LossAB: \u001b[91m3.41(+0.16%) ▲\u001b[0m\t\n",
      "Epoch   117 | LossA: \u001b[32m2.28(-0.64%) ▼\u001b[0m\t| LossAB: \u001b[32m3.22(-5.53%) ▼\u001b[0m\t\n",
      "Epoch   118 | LossA: \u001b[32m2.27(-0.51%) ▼\u001b[0m\t| LossAB: \u001b[91m3.54(+9.72%) ▲\u001b[0m\t\n",
      "Epoch   119 | LossA: \u001b[32m2.26(-0.38%) ▼\u001b[0m\t| LossAB: \u001b[91m3.78(+6.87%) ▲\u001b[0m\t\n",
      "Epoch   120 | LossA: \u001b[91m2.26(+0.11%) ▲\u001b[0m\t| LossAB: \u001b[91m4.40(+16.33%) ▲\u001b[0m\t\n",
      "Epoch   121 | LossA: \u001b[32m2.26(-0.00%) ▼\u001b[0m\t| LossAB: \u001b[32m3.16(-28.24%) ▼\u001b[0m\t\n",
      "Epoch   122 | LossA: \u001b[32m2.24(-0.77%) ▼\u001b[0m\t| LossAB: \u001b[91m4.18(+32.32%) ▲\u001b[0m\t\n",
      "Epoch   123 | LossA: \u001b[32m2.24(-0.41%) ▼\u001b[0m\t| LossAB: \u001b[32m2.94(-29.50%) ▼\u001b[0m\t\n",
      "Epoch   124 | LossA: \u001b[91m2.24(+0.07%) ▲\u001b[0m\t| LossAB: \u001b[91m3.50(+18.91%) ▲\u001b[0m\t\n",
      "Epoch   125 | LossA: \u001b[32m2.23(-0.43%) ▼\u001b[0m\t| LossAB: \u001b[91m3.72(+6.15%) ▲\u001b[0m\t\n",
      "Epoch   126 | LossA: \u001b[32m2.23(-0.10%) ▼\u001b[0m\t| LossAB: \u001b[91m4.22(+13.42%) ▲\u001b[0m\t\n",
      "Epoch   127 | LossA: \u001b[91m2.28(+2.64%) ▲\u001b[0m\t| LossAB: \u001b[32m2.83(-32.83%) ▼\u001b[0m\t\n",
      "Epoch   128 | LossA: \u001b[32m2.25(-1.37%) ▼\u001b[0m\t| LossAB: \u001b[91m3.73(+31.88%) ▲\u001b[0m\t\n",
      "Epoch   129 | LossA: \u001b[32m2.23(-1.11%) ▼\u001b[0m\t| LossAB: \u001b[32m3.25(-12.96%) ▼\u001b[0m\t\n",
      "Epoch   130 | LossA: \u001b[32m2.21(-0.77%) ▼\u001b[0m\t| LossAB: \u001b[91m3.44(+5.70%) ▲\u001b[0m\t\n",
      "Epoch   131 | LossA: \u001b[32m2.20(-0.29%) ▼\u001b[0m\t| LossAB: \u001b[32m2.62(-23.79%) ▼\u001b[0m\t\n",
      "Epoch   132 | LossA: \u001b[32m2.20(-0.13%) ▼\u001b[0m\t| LossAB: \u001b[91m2.80(+6.84%) ▲\u001b[0m\t\n",
      "Epoch   133 | LossA: \u001b[91m2.22(+0.71%) ▲\u001b[0m\t| LossAB: \u001b[91m3.82(+36.42%) ▲\u001b[0m\t\n",
      "Epoch   134 | LossA: \u001b[32m2.19(-0.97%) ▼\u001b[0m\t| LossAB: \u001b[91m4.07(+6.58%) ▲\u001b[0m\t\n",
      "Epoch   135 | LossA: \u001b[32m2.19(-0.03%) ▼\u001b[0m\t| LossAB: \u001b[32m2.81(-30.90%) ▼\u001b[0m\t\n",
      "Epoch   136 | LossA: \u001b[32m2.17(-1.26%) ▼\u001b[0m\t| LossAB: \u001b[91m3.67(+30.60%) ▲\u001b[0m\t\n",
      "Epoch   137 | LossA: \u001b[32m2.16(-0.27%) ▼\u001b[0m\t| LossAB: \u001b[32m3.58(-2.34%) ▼\u001b[0m\t\n",
      "Epoch   138 | LossA: \u001b[32m2.16(-0.05%) ▼\u001b[0m\t| LossAB: \u001b[91m3.73(+4.18%) ▲\u001b[0m\t\n",
      "Epoch   139 | LossA: \u001b[32m2.15(-0.39%) ▼\u001b[0m\t| LossAB: \u001b[32m2.33(-37.67%) ▼\u001b[0m\t\n",
      "Epoch   140 | LossA: \u001b[32m2.15(-0.29%) ▼\u001b[0m\t| LossAB: \u001b[91m2.97(+27.61%) ▲\u001b[0m\t\n",
      "Epoch   141 | LossA: \u001b[32m2.13(-0.52%) ▼\u001b[0m\t| LossAB: \u001b[91m4.36(+46.84%) ▲\u001b[0m\t\n",
      "Epoch   142 | LossA: \u001b[91m2.14(+0.12%) ▲\u001b[0m\t| LossAB: \u001b[32m2.80(-35.81%) ▼\u001b[0m\t\n",
      "Epoch   143 | LossA: \u001b[32m2.13(-0.40%) ▼\u001b[0m\t| LossAB: \u001b[91m3.21(+14.74%) ▲\u001b[0m\t\n",
      "Epoch   144 | LossA: \u001b[91m2.13(+0.02%) ▲\u001b[0m\t| LossAB: \u001b[32m2.73(-15.12%) ▼\u001b[0m\t\n",
      "Epoch   145 | LossA: \u001b[32m2.11(-0.72%) ▼\u001b[0m\t| LossAB: \u001b[91m2.92(+6.93%) ▲\u001b[0m\t\n",
      "Epoch   146 | LossA: \u001b[32m2.11(-0.25%) ▼\u001b[0m\t| LossAB: \u001b[91m3.89(+33.44%) ▲\u001b[0m\t\n",
      "Epoch   147 | LossA: \u001b[32m2.10(-0.24%) ▼\u001b[0m\t| LossAB: \u001b[32m3.63(-6.77%) ▼\u001b[0m\t\n",
      "Epoch   148 | LossA: \u001b[32m2.10(-0.30%) ▼\u001b[0m\t| LossAB: \u001b[32m3.51(-3.31%) ▼\u001b[0m\t\n",
      "Epoch   149 | LossA: \u001b[32m2.09(-0.22%) ▼\u001b[0m\t| LossAB: \u001b[91m4.49(+28.10%) ▲\u001b[0m\t\n",
      "Epoch   150 | LossA: \u001b[32m2.09(-0.32%) ▼\u001b[0m\t| LossAB: \u001b[32m2.62(-41.76%) ▼\u001b[0m\t\n",
      "Epoch   151 | LossA: \u001b[32m2.08(-0.15%) ▼\u001b[0m\t| LossAB: \u001b[91m3.48(+32.87%) ▲\u001b[0m\t\n",
      "Epoch   152 | LossA: \u001b[32m2.08(-0.17%) ▼\u001b[0m\t| LossAB: \u001b[91m4.25(+22.37%) ▲\u001b[0m\t\n",
      "Epoch   153 | LossA: \u001b[32m2.07(-0.31%) ▼\u001b[0m\t| LossAB: \u001b[32m2.48(-41.78%) ▼\u001b[0m\t\n",
      "Epoch   154 | LossA: \u001b[32m2.07(-0.26%) ▼\u001b[0m\t| LossAB: \u001b[91m3.16(+27.51%) ▲\u001b[0m\t\n",
      "Epoch   155 | LossA: \u001b[32m2.06(-0.30%) ▼\u001b[0m\t| LossAB: \u001b[91m3.25(+2.89%) ▲\u001b[0m\t\n",
      "Epoch   156 | LossA: \u001b[32m2.06(-0.14%) ▼\u001b[0m\t| LossAB: \u001b[32m2.71(-16.69%) ▼\u001b[0m\t\n",
      "Epoch   157 | LossA: \u001b[32m2.05(-0.41%) ▼\u001b[0m\t| LossAB: \u001b[91m3.07(+13.53%) ▲\u001b[0m\t\n",
      "Epoch   158 | LossA: \u001b[32m2.04(-0.25%) ▼\u001b[0m\t| LossAB: \u001b[32m2.37(-22.97%) ▼\u001b[0m\t\n",
      "Epoch   159 | LossA: \u001b[32m2.04(-0.28%) ▼\u001b[0m\t| LossAB: \u001b[91m2.78(+17.40%) ▲\u001b[0m\t\n",
      "Epoch   160 | LossA: \u001b[32m2.03(-0.27%) ▼\u001b[0m\t| LossAB: \u001b[91m2.88(+3.50%) ▲\u001b[0m\t\n",
      "Epoch   161 | LossA: \u001b[91m2.04(+0.15%) ▲\u001b[0m\t| LossAB: \u001b[91m3.22(+12.04%) ▲\u001b[0m\t\n",
      "Epoch   162 | LossA: \u001b[32m2.03(-0.34%) ▼\u001b[0m\t| LossAB: \u001b[91m3.45(+6.93%) ▲\u001b[0m\t\n",
      "Epoch   163 | LossA: \u001b[32m2.02(-0.55%) ▼\u001b[0m\t| LossAB: \u001b[32m3.22(-6.60%) ▼\u001b[0m\t\n",
      "Epoch   164 | LossA: \u001b[32m2.02(-0.07%) ▼\u001b[0m\t| LossAB: \u001b[91m3.66(+13.68%) ▲\u001b[0m\t\n",
      "Epoch   165 | LossA: \u001b[91m2.03(+0.48%) ▲\u001b[0m\t| LossAB: \u001b[91m4.09(+11.80%) ▲\u001b[0m\t\n",
      "Epoch   166 | LossA: \u001b[32m2.01(-0.67%) ▼\u001b[0m\t| LossAB: \u001b[32m3.47(-15.15%) ▼\u001b[0m\t\n",
      "Epoch   167 | LossA: \u001b[32m2.01(-0.15%) ▼\u001b[0m\t| LossAB: \u001b[91m3.62(+4.26%) ▲\u001b[0m\t\n",
      "Epoch   168 | LossA: \u001b[32m2.00(-0.51%) ▼\u001b[0m\t| LossAB: \u001b[91m3.86(+6.65%) ▲\u001b[0m\t\n",
      "Epoch   169 | LossA: \u001b[32m1.99(-0.52%) ▼\u001b[0m\t| LossAB: \u001b[32m3.52(-8.80%) ▼\u001b[0m\t\n",
      "Epoch   170 | LossA: \u001b[32m1.99(-0.18%) ▼\u001b[0m\t| LossAB: \u001b[32m2.68(-23.80%) ▼\u001b[0m\t\n",
      "Epoch   171 | LossA: \u001b[32m1.98(-0.16%) ▼\u001b[0m\t| LossAB: \u001b[91m3.16(+17.69%) ▲\u001b[0m\t\n",
      "Epoch   172 | LossA: \u001b[32m1.98(-0.31%) ▼\u001b[0m\t| LossAB: \u001b[91m4.80(+52.18%) ▲\u001b[0m\t\n",
      "Epoch   173 | LossA: \u001b[32m1.97(-0.21%) ▼\u001b[0m\t| LossAB: \u001b[32m3.03(-36.88%) ▼\u001b[0m\t\n",
      "Epoch   174 | LossA: \u001b[32m1.97(-0.19%) ▼\u001b[0m\t| LossAB: \u001b[91m3.37(+10.96%) ▲\u001b[0m\t\n",
      "Epoch   175 | LossA: \u001b[32m1.96(-0.23%) ▼\u001b[0m\t| LossAB: \u001b[32m2.75(-18.40%) ▼\u001b[0m\t\n",
      "Epoch   176 | LossA: \u001b[32m1.96(-0.38%) ▼\u001b[0m\t| LossAB: \u001b[91m2.91(+5.81%) ▲\u001b[0m\t\n",
      "Epoch   177 | LossA: \u001b[32m1.95(-0.28%) ▼\u001b[0m\t| LossAB: \u001b[32m2.38(-18.23%) ▼\u001b[0m\t\n",
      "Epoch   178 | LossA: \u001b[32m1.95(-0.16%) ▼\u001b[0m\t| LossAB: \u001b[91m3.72(+56.50%) ▲\u001b[0m\t\n",
      "Epoch   179 | LossA: \u001b[32m1.94(-0.43%) ▼\u001b[0m\t| LossAB: \u001b[91m4.21(+13.11%) ▲\u001b[0m\t\n",
      "Epoch   180 | LossA: \u001b[32m1.93(-0.30%) ▼\u001b[0m\t| LossAB: \u001b[32m2.72(-35.25%) ▼\u001b[0m\t\n",
      "Epoch   181 | LossA: \u001b[32m1.93(-0.21%) ▼\u001b[0m\t| LossAB: \u001b[32m2.24(-17.87%) ▼\u001b[0m\t\n",
      "Epoch   182 | LossA: \u001b[32m1.92(-0.29%) ▼\u001b[0m\t| LossAB: \u001b[91m2.71(+21.13%) ▲\u001b[0m\t\n",
      "Epoch   183 | LossA: \u001b[32m1.92(-0.27%) ▼\u001b[0m\t| LossAB: \u001b[91m2.94(+8.38%) ▲\u001b[0m\t\n",
      "Epoch   184 | LossA: \u001b[32m1.92(-0.14%) ▼\u001b[0m\t| LossAB: \u001b[91m3.55(+21.03%) ▲\u001b[0m\t\n",
      "Epoch   185 | LossA: \u001b[32m1.91(-0.24%) ▼\u001b[0m\t| LossAB: \u001b[91m3.57(+0.45%) ▲\u001b[0m\t\n",
      "Epoch   186 | LossA: \u001b[32m1.90(-0.35%) ▼\u001b[0m\t| LossAB: \u001b[91m3.66(+2.47%) ▲\u001b[0m\t\n",
      "Epoch   187 | LossA: \u001b[32m1.90(-0.21%) ▼\u001b[0m\t| LossAB: \u001b[32m3.66(-0.03%) ▼\u001b[0m\t\n",
      "Epoch   188 | LossA: \u001b[32m1.90(-0.17%) ▼\u001b[0m\t| LossAB: \u001b[32m3.53(-3.50%) ▼\u001b[0m\t\n",
      "Epoch   189 | LossA: \u001b[32m1.89(-0.25%) ▼\u001b[0m\t| LossAB: \u001b[91m4.72(+33.86%) ▲\u001b[0m\t\n",
      "Epoch   190 | LossA: \u001b[32m1.89(-0.09%) ▼\u001b[0m\t| LossAB: \u001b[32m2.46(-47.89%) ▼\u001b[0m\t\n",
      "Epoch   191 | LossA: \u001b[32m1.88(-0.36%) ▼\u001b[0m\t| LossAB: \u001b[91m4.57(+85.60%) ▲\u001b[0m\t\n",
      "Epoch   192 | LossA: \u001b[32m1.88(-0.41%) ▼\u001b[0m\t| LossAB: \u001b[32m4.24(-7.13%) ▼\u001b[0m\t\n",
      "Epoch   193 | LossA: \u001b[32m1.88(-0.01%) ▼\u001b[0m\t| LossAB: \u001b[32m3.10(-26.86%) ▼\u001b[0m\t\n",
      "Epoch   194 | LossA: \u001b[32m1.87(-0.41%) ▼\u001b[0m\t| LossAB: \u001b[91m3.29(+6.07%) ▲\u001b[0m\t\n",
      "Epoch   195 | LossA: \u001b[32m1.87(-0.08%) ▼\u001b[0m\t| LossAB: \u001b[91m3.30(+0.26%) ▲\u001b[0m\t\n",
      "Epoch   196 | LossA: \u001b[32m1.86(-0.29%) ▼\u001b[0m\t| LossAB: \u001b[32m2.27(-31.29%) ▼\u001b[0m\t\n",
      "Epoch   197 | LossA: \u001b[32m1.85(-0.37%) ▼\u001b[0m\t| LossAB: \u001b[91m2.80(+23.47%) ▲\u001b[0m\t\n",
      "Epoch   198 | LossA: \u001b[32m1.85(-0.23%) ▼\u001b[0m\t| LossAB: \u001b[91m3.74(+33.43%) ▲\u001b[0m\t\n",
      "637/637 [==============================] - 7s 11ms/step\n",
      "Accuracy: 0.7503924369812012\n"
     ]
    }
   ],
   "source": [
    "# visualizing losses and accuracy\n",
    "train_loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "y_test_oh = dense_to_one_hot(y_test,num_clases=4)\n",
    "\n",
    "#Observing the losses but can be commented out as it's not mandatory \n",
    "reporter = lossprettifier.LossPrettifier(show_percentage=True)\n",
    "\n",
    "for i in range(numEpochs-1):\n",
    "    reporter(epoch=i, LossA = train_loss[i], LossAB = val_loss[i])\n",
    "\n",
    "# Model evaluation \n",
    "score, acc = model.evaluate(x_test, y_test_oh, batch_size=batch_Size)\n",
    "print(\"Accuracy:\", acc)\n",
    "\n",
    "#if acc>0.675:\n",
    "model.save_weights(modelPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.95      0.59      0.73       234\n",
      "           1       0.76      0.93      0.84       246\n",
      "           2       0.59      0.70      0.64       149\n",
      "           3       0.45      0.62      0.53         8\n",
      "\n",
      "    accuracy                           0.75       637\n",
      "   macro avg       0.69      0.71      0.68       637\n",
      "weighted avg       0.79      0.75      0.75       637\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(x_test)\n",
    "#y_pred = y_pred.reshape(len(y_test), 4)\n",
    "y_pred = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Writing results on file\n",
    "f = open(resultPath,'a') #create classification report\n",
    "f.write(classification_report(y_test, y_pred))\n",
    "f.write(str(sklm.cohen_kappa_score(y_test, y_pred))+\",\"+str(acc)+\",\"+str(score)+\"\\n\")\n",
    "\n",
    "#Print class-wise classification metrics\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
